---
execute:
  echo: false
---

# WorldClim 2.1 data download

::: {.callout-important}
Careful! This may take a while.
:::

```{r}
#| eval: false

library(beepr, quietly = TRUE)
library(checkmate, quietly = TRUE)
library(cli, quietly = TRUE)
library(curl, quietly = TRUE)
library(here, quietly = TRUE)
library(httr, quietly = TRUE)
library(magrittr, quietly = TRUE)
library(osfr, quietly = TRUE)
library(purrr, quietly = TRUE)
library(rvest, quietly = TRUE)
library(rutils, quietly = TRUE) # danielvartan/rutils (GitHub)
library(rstudioapi, quietly = TRUE)
library(stringr, quietly = TRUE)
library(utils, quietly = TRUE)
library(zip, quietly = TRUE)
```

## Preset

```{r}
# dir <- rstudioapi::selectDirectory()
dir <- here::here("data-raw")
if (!checkmate::test_directory_exists(dir)) dir.create(dir)
```

## Web scraping

```{r}
source <- "https://worldclim.org/data/monthlywth.html"
html <- source |> rvest::read_html()
```

## Extract URLs

```{r}
urls <- html |>
  rvest::html_elements("a") |>
  rvest::html_attr("href") |>
  stringr::str_subset("geodata")
```

## Get content length

```{r}
get_content_length <- function(url) {
  url_pattern <- paste0(
    "(http[s]?|ftp)://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|",
    "(?:%[0-9a-fA-F][0-9a-fA-F]))+"
  )

  checkmate::assert_string(url, pattern = url_pattern)

  request <- try({url |> httr::HEAD()}, silent = TRUE)
  
  if (class(request) == "try-error") {
    as.numeric(NA)
  } else if (!is.null(request$headers$`Content-Length`)) {
    as.numeric(request$headers$`content-length`)
  } else {
    as.numeric(NA)
  }
}
```

```{r}
sizes <-
  urls |>
  purrr::map_dbl(
    .f = get_content_length,
    .progress = TRUE
)

beepr::beep(1)
```

## Organize the metadata

```{r}
metadata <- dplyr::tibble(
  url = urls,
  file = basename(urls),
  size = as.numeric(sizes),
  size_cum_sum = cumsum(dplyr::coalesce(size, 0)) + (size * 0)
)
```

## Check for errors

```{r}
{
  cli::cli_alert_info(
    paste0(
      "{.strong {cli::col_red(rutils:::count_na(metadata$size))}} ",
      "url requests resulted in error."
    )
  )

  if (rutils:::count_na(metadata$size) > 0) {
    cli::cli_alert_info("Their file names are:")
    cli::cli_li(metadata$file[is.na(metadata$size)])
  }
}
```

```{r}
#| eval: false

utils::writeClipboard(metadata$file[is.na(metadata$size)])
```

## Download batch

```{r}
models <- 
  metadata |>
  magrittr::extract2("file") |>
  stringr::str_extract("(?<=(bioc_|tmin_|tmax_|prec_)).*(?=_ssp)") |>
  unique()
```

```{r}
download_batch <- function(
    metadata, dir, model = NULL, max_batch_size = NULL, rows = NULL
  ) {
  checkmate::assert_tibble(metadata)
  checkmate::assert_subset(c("url", "size_cum_sum"), colnames(metadata))
  checkmate::assert_directory_exists(dir, access = "w")
  checkmate::assert_integerish(max_batch_size, lower = 1, null.ok = TRUE)
  checkmate::assert_numeric(rows, lower = 1, null.ok = TRUE)

  models <- 
    metadata |>
    magrittr::extract2("file") |>
    stringr::str_extract("(?<=(bioc_|tmin_|tmax_|prec_)).*(?=_ssp)") |>
    unique()
  
  checkmate::assert_choice(model, models, null.ok = TRUE)
  
  if (!is.null(model)) {
    metadata <- metadata |> dplyr::filter(stringr::str_detect(file, model))
  }
  
  if (!is.null(max_batch_size)) {
    metadata <- metadata |> dplyr::filter(size_cum_sum <= max_batch_size)
  }

  if (!is.null(rows)) {metadata <- metadata |> dplyr::slice(rows)}
  if (nrow(metadata) == 0) {cli::cli_abort("No files to download.")}

  cli::cli_alert_info(
    paste0(
      "Downloading ",
      "{.strong {cli::col_red(nrow(metadata))}} ",
      "{cli::qty(nrow(metadata))} ",
      "file{?s} to {.strong {dir}}.")
  )

  if (nrow(metadata) > 1) cli::cat_line()

  cli::cli_progress_bar(
    name = "Downloading",
    total = nrow(metadata),
    clear = FALSE
  )

  broken_links <- character()
  
  for (i in metadata$url) {
    test <- try(
      i |>
        curl::curl_download(
          destfile = file.path(dir, basename(i)),
          quiet = TRUE
        ),
      silent = TRUE
    )

    if (class(test) == "try-error") {
      cli::cli_alert_info(
        paste0(
          "The file {.strong {basename(i)}} could not be downloaded."
        )
      )
      
      broken_links <- broken_links |> append(i)
    }

    cli::cli_progress_update()
  }

  cli::cli_progress_done()
  invisible(broken_links)
}
```

```{r}
nrow(metadata)
```

```{r}
models
```

```{r}
model <- NULL # model <- "ACCESS-CM2"
```

```{r}
broken_links <- metadata |> download_batch(dir = dir, model = model)

beepr::beep(1)
```

```{r}
# Use only if the download process was interrupted.

downloaded_files <- 
  list.files(dir) |>
  setdiff(list.dirs(dir, full.names = FALSE))

broken_links <- 
  metadata |>
  # (just for future climate data)
  dplyr::filter(stringr::str_detect(file, model)) |>
  dplyr::filter(!file %in% downloaded_files) |>
  download_batch(dir = dir)

beepr::beep(1)
```

## ZIP files (just for future climate data)

```{r}
options(cli.progress_show_after = 0)

zip_files <- function(
    metadata, model, dir, suffix, broken_links = NULL, engine = "zip"
  ) {
  checkmate::assert_tibble(metadata)
  checkmate::assert_subset(c("file", "size"), colnames(metadata))
  checkmate::assert_directory_exists(dir, access = "w")
  checkmate::assert_string(suffix)
  checkmate::assert_character(broken_links, null.ok = TRUE)
  checkmate::assert_choice(engine, c("utils", "zip"))
  
  models <- 
    metadata |>
    magrittr::extract2("file") |>
    stringr::str_extract("(?<=(bioc_|tmin_|tmax_|prec_)).*(?=_ssp)") |>
    unique()
  
  checkmate::assert_subset(model, models)
  
  zip_dir <- file.path(dir, "zip")
  if (!checkmate::test_directory_exists(zip_dir)) {dir.create(zip_dir)}
  
  cli::cli_alert_info(
    paste0(
      "Compressing files for ",
      "{.strong {cli::col_red(length(model))}}", 
      "{cli::qty(length(model))} ", 
      "model{?s}: ",
      "{.strong {model}}."
    )
  )
  
  for (i in model) {
    files <- 
      metadata |>
      magrittr::extract2("file") |>
      stringr::str_subset(i)
    
    files <- files |> magrittr::extract(!files %in% broken_links)
    files <- files |> magrittr::extract(!is.na(files))
    
    if (length(files) == 0) {
      cli::cli_alert_info(
        "The model {.strong {cli::col_red(i)}} do not have any files."
      )
      
      next
    }
    
    total_size <- 
      metadata |> 
      dplyr::filter(file %in% files) |> 
      magrittr::extract2("size") |> 
      sum()
    
    if (total_size > 5e9) {
      file_chunks <- 
        files |> 
        split(cut(seq_along(files), ceiling(total_size  / 5e9)))
    } else {
      file_chunks <- list(files)
    }
    
    cli::cli_alert_info(
      paste0(
        "Compressing files for the ",
        "{.strong {cli::col_red(i)}} model in ",
        "{.strong {length(file_chunks)}}",
        "{cli::qty(length(file_chunks))} ", 
        "chunk{?s}."
      )
    )
    
    cli::cli_progress_bar(
      name = "Compressing chunks",
      total = length(file_chunks),
      type = "tasks",
      clear = FALSE
    )
    
    for (j in seq_along(file_chunks)) {
      zip_file_name <- 
        ifelse(
          length(file_chunks) == 1, 
          file.path(zip_dir, paste0(i, suffix, ".zip")),
          file.path(zip_dir, paste0(i, suffix, "_", j, ".zip"))
        )
      
      if (engine == "zip") {
        zip::zip(
          zipfile = zip_file_name,
          files = file.path(dir, file_chunks[[j]]),
          include_directories = FALSE,
          mode = "cherry-pick"
        )
      } else {
        # Use `zip -h` on Window's command prompt to see the flags.
        utils::zip(
          zipfile = zip_file_name,
          files = file.path(dir, file_chunks[[j]]),
          flags = "-jq"
        )
      }
      
      cli::cli_progress_update()
    }
    
    cli::cli_progress_done()
  }
  
  invisible(NULL)
}
```

```{r}
#| output: false

# broken_links <- character()
# broken_links <- utils::readClipboard()
# model <- models
# model <- c("IPSL-CM6A-LR", "MIROC6", "UKESM1-0-LL")

suffix <- "_2-5m"

metadata |> 
  zip_files(
    model = model, 
    dir = dir, 
    suffix = suffix, 
    broken_links = broken_links,
    engine = "utils" # "zip"
  )

beepr::beep(1)
```

## Delete downloaded files

```{r}
#| eval: false

# Careful! This routine should be use only in special cases.

for (i in model) {
  list.files(dir, full.names = TRUE) |>
    stringr::str_subset(i) |>
    file.remove()
}

invisible(gc())
```
