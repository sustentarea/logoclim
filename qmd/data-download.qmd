---
execute:
  eval: false
---

<!-- To do: Move these algorithms to a `logoclim` R package. -->

# WorldClim 2.1: Data Download

## Overview

This document provides a step-by-step guide to download the WorldClim 2.1 data from the [WorldClim website](https://worldclim.org/data/monthlywth.html).

::: {.callout-note}
Due to UC Davis server constraints, it's much faster to download WorldClim 2.1 raw data from the `LogoClim` research compendium on the Open Science Framework (OSF). Click [here](https://osf.io/2bpjh/) to access this data.

This notebook should be used if you want to download the data directly from the WorldClim website.
:::

::: {.callout-important}
This process may take some time to complete. Please be patient.
:::

## Setting the Environment

```{r}
#| eval: false

library(beepr)
library(checkmate)
library(cli)
library(curl)
library(dplyr)
library(fs)
library(here)
library(httr)
library(magrittr)
library(purrr)
library(rutils) # github.com/danielvartan/rutils (GitHub)
library(rvest)
library(stringr)
library(utils)
library(zip)
```

```{r}
source(here::here("R", "download_wc_files.R"))
source(here::here("R", "get_content_length.R"))
source(here::here("R", "zip_wc_files.R"))
```

## Setting Variables

### Options

```{r}
options(cli.progress_show_after = 0)
```

### Series

Pick one of the following series:

```{r}
series <- "historical-climate-data"
```

```{r}
#| eval: false

series <- "historical-monthly-weather-data"
```

```{r}
#| eval: false

series <- "future-climate-data"
```

### Directories

```{r}
raw_data_dir <- here::here("data-raw")
```

```{r}
wc_raw_data_dir <- fs::path(raw_data_dir, "worldclim")
```

```{r}
wc_raw_data_series_dir <- fs::path(wc_raw_data_dir, series)
```

```{r}
dirs <- c(raw_data_dir, wc_raw_data_dir, wc_raw_data_series_dir)

for (i in dirs) {
  if (!checkmate::test_directory_exists(i)) {
    fs::dir_create(i, recurse = TRUE)
  }
}
```

### Parameters

```{r}
resolution <- "5m" # all, 10m, 5m, 2.5m, 30s
```

## Scrapping the Source

```{r}
html <-
  get_wc_2_1_url(series, resolution) |>
  rvest::read_html()
```

```{r}
urls <-
  html |>
  rvest::html_elements("a") |>
  rvest::html_attr("href") |>
  stringr::str_subset("geodata")
```

```{r}
if (!resolution == "all") {
  urls <-
    urls %>%
    magrittr::extract(
      stringr::str_detect(
        basename(.),
        paste0("(?<=_)", resolution)
      )
    )
}
```

## Creating the Metadata

```{r}
sizes <-
  urls |>
  purrr::map_dbl(
    .f = get_content_length,
    .progress = TRUE
)

beepr::beep(1)
```

```{r}
metadata <- dplyr::tibble(
  url = urls,
  file = basename(urls),
  size = as.numeric(sizes), # In bytes # 1 megabyte = 2^20 bytes
  size_cum_sum = cumsum(dplyr::coalesce(size, 0)) + (size * 0)
)
```

```{r}
metadata |> dplyr::glimpse()
```

## Checking for Errors

```{r}
{
  cli::cli_alert_info(
    paste0(
      "{.strong {cli::col_red(rutils:::count_na(metadata$size))}} ",
      "url requests resulted in error."
    )
  )

  if (rutils::count_na(metadata$size) > 0) {
    cli::cli_alert_info("Their file names are:")
    cli::cli_li(metadata$file[is.na(metadata$size)])
  }
}
```

## Defining the Model

This is only useful when dealing with future climate data.

Run the `model <- NULL` code chunk when downloading other series.

```{r}
model <- NULL
```

```{r}
#| eval: false

models <-
  metadata |>
  magrittr::extract2("file") |>
  stringr::str_extract("(?<=(bioc_|tmin_|tmax_|prec_)).*(?=_ssp)") |>
  unique()

models
```

```{r}
#| eval: false

model <- "ACCESS-CM2"
# model <- c("IPSL-CM6A-LR", "MIROC6", "UKESM1-0-LL")
# model <- models
```

## Downloading the Files

```{r}
#| eval: false

# zip_dir <- fs::path(wc_raw_data_series_dir, "tif")
zip_dir <- fs::path(wc_raw_data_series_dir, "zip")

if (!checkmate::test_directory_exists(zip_dir)) {
  zip_dir |> fs::dir_create(recurse = TRUE)
}
```

```{r}
broken_links <-
  metadata |>
  download_wc_files(dir = zip_dir, model = model)

beepr::beep(1)
```

Run the code chunk below only if the download process was interrupted.

```{r}
#| eval: false

downloaded_files <-
  fs::path(wc_raw_data_series_dir, "zip") |>
  fs::dir_ls(type = "file")

broken_links <-
  metadata |>
  # dplyr::filter(stringr::str_detect(file, model)) |>
  dplyr::filter(!file %in% basename(downloaded_files)) |>
  download_wc_files(dir = file.path(wc_raw_data_series_dir, "zip"))

beepr::beep(1)
```

## Compress files (Optional)

This is only useful when dealing with future climate data.

```{r}
#| eval: false
#| output: false

metadata |>
  zip_wc_files(
    model = model,
    dir = wc_raw_data_series_dir,
    broken_links = broken_links
  )

beepr::beep(1)
```

## Delete TIF Files (Optional)

This is only useful when dealing with future climate data.

```{r}
#| eval: false

for (i in model) {
  fs::dir_ls(zip_dir, type = "file", regexp = i) |>
    fs::file_delete()
}

invisible(gc())
```
